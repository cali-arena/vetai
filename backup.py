"""
VetDiagnosisAI - Sistema Inteligente de Apoio ao DiagnÃ³stico VeterinÃ¡rio
AplicaÃ§Ã£o principal Streamlit
"""

import streamlit as st
import pandas as pd
import numpy as np
import pickle
import joblib
from pathlib import Path
from datetime import datetime
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.decomposition import PCA
import plotly.express as px
import plotly.graph_objects as go

# ConfiguraÃ§Ã£o da pÃ¡gina
st.set_page_config(
    page_title="VetDiagnosisAI",
    page_icon="ğŸ¾",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS customizado
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
        padding: 1rem;
        border-radius: 0.5rem;
    }
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
        padding: 1rem;
        border-radius: 0.5rem;
    }
    .error-box {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        color: #721c24;
        padding: 1rem;
        border-radius: 0.5rem;
    }
</style>
""", unsafe_allow_html=True)

# InicializaÃ§Ã£o do session_state
if 'df_main' not in st.session_state:
    st.session_state.df_main = None
if 'modelo_treinado' not in st.session_state:
    st.session_state.modelo_treinado = None
if 'preprocessor' not in st.session_state:
    st.session_state.preprocessor = None
if 'feature_names' not in st.session_state:
    st.session_state.feature_names = None
if 'target_names' not in st.session_state:
    st.session_state.target_names = None

# FunÃ§Ã£o removida - APENAS dados reais serÃ£o usados

# FunÃ§Ã£o removida - carregamento direto implementado abaixo

# FORÃ‡AR CARREGAMENTO DE DADOS - SEMPRE!
st.info("ğŸ”„ Inicializando sistema...")

# Tentar carregar dados reais primeiro, depois fallback para incorporados
df_real = None
dataset_source = ""

# 1. Tentar carregar dados reais da pasta data
try:
    # Tentar mÃºltiplos caminhos possÃ­veis
    possible_paths = [
        Path("data"),           # Para execuÃ§Ã£o local
        Path("VET/data"),       # Para execuÃ§Ã£o no Streamlit Cloud
        Path(".") / "data",     # Caminho relativo
        Path(".") / "VET" / "data"  # Caminho relativo com VET
    ]
    
    data_path = None
    for path in possible_paths:
        if path.exists():
            csv_files = list(path.glob("*.csv"))
            if csv_files:
                data_path = path
                st.info(f"ğŸ“ Encontrada pasta de dados: {path}")
                break
    
    if data_path and data_path.exists():
        csv_files = list(data_path.glob("*.csv"))
        if csv_files:
            # Priorizar datasets reais especÃ­ficos
            datasets_prioritarios = [
                'veterinary_complete_real_dataset.csv',
                'veterinary_master_dataset.csv', 
                'veterinary_realistic_dataset.csv',
                'clinical_veterinary_data.csv',
                'laboratory_complete_panel.csv'
            ]
            
            for dataset_name in datasets_prioritarios:
                dataset_path = data_path / dataset_name
                if dataset_path.exists():
                    df_real = pd.read_csv(dataset_path)
                    if df_real is not None and len(df_real) > 0:
                        dataset_source = f"dados_reais_{dataset_name}"
                        st.success(f"âœ… Carregado dataset real: {dataset_name} ({len(df_real)} registros)")
                        break
except Exception as e:
    st.warning(f"âš ï¸ Erro ao carregar dados reais: {e}")

# 2. APENAS dados reais - SEM fallback para sintÃ©ticos
if df_real is not None and len(df_real) > 0:
    # SEMPRE definir os dados no session state
    st.session_state.df_main = df_real
    st.session_state.dataset_carregado_auto = True
    st.session_state.dataset_sempre_carregado = True
    st.session_state.dados_prontos = True
    st.session_state.dataset_source = dataset_source
    
    # Adicionar informaÃ§Ãµes de debug
    import datetime
    st.session_state.dataset_timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    st.success(f"âœ… Sistema inicializado com {len(df_real)} registros de {dataset_source}!")
else:
    st.session_state.dados_prontos = False
    st.error("âŒ ERRO: Nenhum dataset real encontrado!")
    st.error("ğŸ“ Verifique se existem arquivos CSV nas seguintes pastas:")
    
    # Verificar todos os caminhos possÃ­veis
    possible_paths = [
        Path("data"),           
        Path("VET/data"),       
        Path(".") / "data",     
        Path(".") / "VET" / "data"  
    ]
    
    found_files = False
    for data_path in possible_paths:
        if data_path.exists():
            csv_files = list(data_path.glob("*.csv"))
            if csv_files:
                st.info(f"ğŸ“‹ Arquivos encontrados na pasta {data_path}:")
                for file in csv_files:
                    st.write(f"  - {file.name}")
                found_files = True
            else:
                st.warning(f"âš ï¸ Pasta '{data_path}' existe mas nÃ£o contÃ©m arquivos CSV")
        else:
            st.warning(f"âš ï¸ Pasta '{data_path}' nÃ£o encontrada")
    
    if not found_files:
        st.info("ğŸ’¡ Para usar o sistema, adicione datasets reais nas seguintes pastas com os seguintes nomes:")
        st.write("ğŸ“ Caminhos possÃ­veis:")
        st.write("- data/")
        st.write("- VET/data/")
        st.write("ğŸ“‹ Arquivos necessÃ¡rios:")
        st.write("- veterinary_complete_real_dataset.csv")
        st.write("- veterinary_master_dataset.csv")
        st.write("- veterinary_realistic_dataset.csv")
        st.write("- clinical_veterinary_data.csv")
        st.write("- laboratory_complete_panel.csv")
    
    st.stop()

# Sidebar com informaÃ§Ãµes
with st.sidebar:
    st.image("https://img.icons8.com/color/96/000000/veterinarian.png", width=100)
    st.title("NavegaÃ§Ã£o")
    
    # InformaÃ§Ãµes do dataset carregado
    if st.session_state.df_main is not None:
        st.success(f"ğŸ“Š Dataset: {len(st.session_state.df_main)} registros")
        if hasattr(st.session_state, 'dataset_source'):
            st.info(f"ğŸ“ Fonte: {st.session_state.dataset_source}")
        if hasattr(st.session_state, 'dataset_timestamp'):
            st.info(f"ğŸ•’ Carregado: {st.session_state.dataset_timestamp}")
    
    # NavegaÃ§Ã£o por pÃ¡ginas
    pagina = st.selectbox(
        "Selecione a pÃ¡gina:",
        ["ğŸ  VisÃ£o Geral", "ğŸ“Š AnÃ¡lise de Dados", "ğŸ¤– Treinar Modelo", "ğŸ” PrediÃ§Ã£o", "ğŸ“ˆ EstatÃ­sticas", "ğŸ“ InformaÃ§Ãµes do Dataset"]
    )

# TÃ­tulo principal
st.markdown('<h1 class="main-header">ğŸ¾ VetDiagnosisAI</h1>', unsafe_allow_html=True)
st.markdown('<p style="text-align: center; font-size: 1.2rem; color: #666;">Sistema Inteligente de Apoio ao DiagnÃ³stico VeterinÃ¡rio</p>', unsafe_allow_html=True)

# Verificar se os dados estÃ£o carregados
if st.session_state.df_main is None:
    st.error("âŒ Nenhum dataset carregado. Por favor, verifique os arquivos de dados.")
    st.stop()

df = st.session_state.df_main

# NavegaÃ§Ã£o por pÃ¡ginas
if pagina == "ğŸ  VisÃ£o Geral":
    st.header("ğŸ“Š VisÃ£o Geral do Sistema")
    
    # MÃ©tricas principais
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total de Registros", len(df))
    
    with col2:
        especies = df['especie'].nunique() if 'especie' in df.columns else 0
        st.metric("EspÃ©cies", especies)
    
    with col3:
        diagnosticos = df['diagnostico'].nunique() if 'diagnostico' in df.columns else 0
        st.metric("DiagnÃ³sticos", diagnosticos)
    
    with col4:
        colunas = len(df.columns)
        st.metric("VariÃ¡veis", colunas)
    
    # DistribuiÃ§Ã£o por espÃ©cie
    if 'especie' in df.columns:
        st.subheader("ğŸ“Š DistribuiÃ§Ã£o por EspÃ©cie")
        especie_counts = df['especie'].value_counts()
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            fig = px.pie(values=especie_counts.values, names=especie_counts.index, 
                        title="DistribuiÃ§Ã£o por EspÃ©cie")
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.dataframe(especie_counts.reset_index().rename(columns={'index': 'EspÃ©cie', 'especie': 'Quantidade'}))
    
    # DistribuiÃ§Ã£o de diagnÃ³sticos
    if 'diagnostico' in df.columns:
        st.subheader("ğŸ¥ DistribuiÃ§Ã£o de DiagnÃ³sticos")
        diag_counts = df['diagnostico'].value_counts().head(10)
        
        fig = px.bar(x=diag_counts.values, y=diag_counts.index, 
                    title="Top 10 DiagnÃ³sticos",
                    orientation='h')
        fig.update_layout(yaxis={'categoryorder': 'total ascending'})
        st.plotly_chart(fig, use_container_width=True)

elif pagina == "ğŸ“Š AnÃ¡lise de Dados":
    st.header("ğŸ“Š AnÃ¡lise Detalhada dos Dados")
    
    # Filtros
    st.subheader("ğŸ” Filtros")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if 'especie' in df.columns:
            especies_filtro = st.multiselect("EspÃ©cie", df['especie'].unique(), default=df['especie'].unique())
        else:
            especies_filtro = []
    
    with col2:
        if 'idade_anos' in df.columns:
            idade_range = st.slider("Idade (anos)", 
                                  float(df['idade_anos'].min()), 
                                  float(df['idade_anos'].max()), 
                                  (float(df['idade_anos'].min()), float(df['idade_anos'].max())))
        else:
            idade_range = (0, 20)
    
    with col3:
        if 'diagnostico' in df.columns:
            diag_filtro = st.multiselect("DiagnÃ³stico", df['diagnostico'].unique(), default=df['diagnostico'].unique())
        else:
            diag_filtro = []
    
    # Aplicar filtros
    df_filtrado = df.copy()
    
    if especies_filtro and 'especie' in df.columns:
        df_filtrado = df_filtrado[df_filtrado['especie'].isin(especies_filtro)]
    
    if 'idade_anos' in df.columns:
        df_filtrado = df_filtrado[
            (df_filtrado['idade_anos'] >= idade_range[0]) & 
            (df_filtrado['idade_anos'] <= idade_range[1])
        ]
    
    if diag_filtro and 'diagnostico' in df.columns:
        df_filtrado = df_filtrado[df_filtrado['diagnostico'].isin(diag_filtro)]
    
    st.info(f"ğŸ“Š Mostrando {len(df_filtrado)} registros de {len(df)} totais")
    
    # VisualizaÃ§Ãµes
    if len(df_filtrado) > 0:
        # DistribuiÃ§Ã£o de idade
        if 'idade_anos' in df_filtrado.columns:
            st.subheader("ğŸ“ˆ DistribuiÃ§Ã£o de Idade")
            fig = px.histogram(df_filtrado, x='idade_anos', nbins=20, title="DistribuiÃ§Ã£o de Idade")
            st.plotly_chart(fig, use_container_width=True)
        
        # CorrelaÃ§Ãµes entre variÃ¡veis numÃ©ricas
        numeric_cols = df_filtrado.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 1:
            st.subheader("ğŸ”— Matriz de CorrelaÃ§Ã£o")
            corr_matrix = df_filtrado[numeric_cols].corr()
            
            fig = px.imshow(corr_matrix, 
                           text_auto=True, 
                           aspect="auto",
                           title="Matriz de CorrelaÃ§Ã£o")
            st.plotly_chart(fig, use_container_width=True)
        
        # Tabela de dados
        st.subheader("ğŸ“‹ Dados Filtrados")
        st.dataframe(df_filtrado.head(100), use_container_width=True)

elif pagina == "ğŸ¤– Treinar Modelo":
    st.header("ğŸš€ Gradient Boosting Otimizado - Sistema de Aprendizado ContÃ­nuo")
    
    if st.session_state.df_main is not None:
        df = st.session_state.df_main
        
        # Verificar se temos dados suficientes para ML
        if 'diagnostico' not in df.columns:
            st.error("âŒ Coluna 'diagnostico' nÃ£o encontrada. NÃ£o Ã© possÃ­vel treinar modelos.")
        else:
            st.success(f"âœ… Dados disponÃ­veis: {len(df)} registros")
            
            # Mostrar informaÃ§Ãµes dos dados
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total de Registros", len(df))
            with col2:
                st.metric("DiagnÃ³sticos Ãšnicos", df['diagnostico'].nunique())
            with col3:
                st.metric("Features DisponÃ­veis", len(df.columns))
            
            # Preparar dados para ML
            st.subheader("ğŸ”§ PreparaÃ§Ã£o dos Dados")
            
            # Feature Engineering AvanÃ§ado
            df_ml = df.copy()
            
            # 1. CodificaÃ§Ã£o de variÃ¡veis categÃ³ricas
            le_especie = LabelEncoder()
            le_sexo = LabelEncoder()
            le_diagnostico = LabelEncoder()
            
            if 'especie' in df_ml.columns:
                df_ml['especie_encoded'] = le_especie.fit_transform(df_ml['especie'])
            if 'sexo' in df_ml.columns:
                df_ml['sexo_encoded'] = le_sexo.fit_transform(df_ml['sexo'])
            
            df_ml['diagnostico_encoded'] = le_diagnostico.fit_transform(df_ml['diagnostico'])
            
            # 2. Criar features derivadas avanÃ§adas
            if 'idade_anos' in df_ml.columns:
                try:
                    df_ml['idade_categoria'] = pd.cut(df_ml['idade_anos'], bins=[0, 1, 3, 7, 12, 100], labels=['Filhote', 'Jovem', 'Adulto', 'Maduro', 'Idoso'])
                    df_ml['idade_categoria_encoded'] = LabelEncoder().fit_transform(df_ml['idade_categoria'])
                except Exception as e:
                    st.warning(f"âš ï¸ Erro ao criar categoria de idade: {e}")
                    # Usar categorizaÃ§Ã£o simples como fallback
                    df_ml['idade_categoria_encoded'] = (df_ml['idade_anos'] // 5).astype(int)
                
                # Features de idade
                try:
                    df_ml['idade_quadrado'] = df_ml['idade_anos'] ** 2
                    df_ml['idade_log'] = np.log1p(df_ml['idade_anos'])
                    df_ml['idade_senior'] = (df_ml['idade_anos'] > 7).astype(int)
                    df_ml['idade_filhote'] = (df_ml['idade_anos'] < 1).astype(int)
                except Exception as e:
                    st.warning(f"âš ï¸ Erro ao criar features de idade: {e}")
            
            # 3. Features de exames laboratoriais combinados avanÃ§ados
            exames_cols = ['hemoglobina', 'hematocrito', 'leucocitos', 'glicose', 'ureia', 'creatinina', 'alt', 'ast', 'fosfatase_alcalina', 'proteinas_totais', 'albumina']
            exames_disponiveis = [col for col in exames_cols if col in df_ml.columns]
            
            if len(exames_disponiveis) >= 3:
                # Criar Ã­ndices clÃ­nicos especÃ­ficos
                if 'hemoglobina' in df_ml.columns and 'hematocrito' in df_ml.columns:
                    df_ml['indice_anemia'] = df_ml['hemoglobina'] / (df_ml['hematocrito'] / 3)
                    df_ml['anemia_grave'] = (df_ml['hemoglobina'] < 8).astype(int)
                
                if 'ureia' in df_ml.columns and 'creatinina' in df_ml.columns:
                    df_ml['indice_renal'] = df_ml['ureia'] / df_ml['creatinina']
                    df_ml['insuficiencia_renal'] = ((df_ml['ureia'] > 60) | (df_ml['creatinina'] > 2)).astype(int)
                
                if 'glicose' in df_ml.columns:
                    df_ml['diabetes'] = (df_ml['glicose'] > 150).astype(int)
                    df_ml['hipoglicemia'] = (df_ml['glicose'] < 60).astype(int)
                
                if 'leucocitos' in df_ml.columns:
                    df_ml['leucocitose'] = (df_ml['leucocitos'] > 12000).astype(int)
                    df_ml['leucopenia'] = (df_ml['leucocitos'] < 4000).astype(int)
            
            # 4. Features de sintomas combinados avanÃ§ados
            sintomas_cols = ['febre', 'apatia', 'perda_peso', 'vomito', 'diarreia', 'tosse', 'letargia', 'feridas_cutaneas', 'poliuria', 'polidipsia']
            sintomas_disponiveis = [col for col in sintomas_cols if col in df_ml.columns]
            
            if len(sintomas_disponiveis) >= 2:
                try:
                    df_ml['total_sintomas'] = df_ml[sintomas_disponiveis].sum(axis=1)
                    df_ml['severidade_sintomas'] = pd.cut(df_ml['total_sintomas'], bins=[-1, 0, 1, 3, 5, 10], labels=['AssintomÃ¡tico', 'Leve', 'Moderado', 'Severo', 'CrÃ­tico'])
                    df_ml['severidade_sintomas_encoded'] = LabelEncoder().fit_transform(df_ml['severidade_sintomas'])
                except Exception as e:
                    st.warning(f"âš ï¸ Erro ao criar features de sintomas: {e}")
                    # Fallback simples
                    df_ml['total_sintomas'] = df_ml[sintomas_disponiveis].sum(axis=1)
                    df_ml['severidade_sintomas_encoded'] = (df_ml['total_sintomas'] > 2).astype(int)
                
                # SÃ­ndromes especÃ­ficas
                if all(col in df_ml.columns for col in ['febre', 'tosse']):
                    df_ml['sindrome_respiratoria'] = (df_ml['febre'] & df_ml['tosse']).astype(int)
                
                if all(col in df_ml.columns for col in ['vomito', 'diarreia']):
                    df_ml['sindrome_gastrointestinal'] = (df_ml['vomito'] | df_ml['diarreia']).astype(int)
                
                if all(col in df_ml.columns for col in ['poliuria', 'polidipsia']):
                    df_ml['sindrome_polidipsica'] = (df_ml['poliuria'] & df_ml['polidipsia']).astype(int)
                
                if all(col in df_ml.columns for col in ['apatia', 'perda_peso']):
                    df_ml['sindrome_sistemica'] = (df_ml['apatia'] & df_ml['perda_peso']).astype(int)
            
            # Selecionar features para ML
            feature_cols = []
            
            try:
                # Adicionar colunas numÃ©ricas originais
                numeric_cols = df_ml.select_dtypes(include=[np.number]).columns.tolist()
                feature_cols.extend([col for col in numeric_cols if col not in ['diagnostico_encoded']])
                
                # Remover colunas com muitos valores Ãºnicos (como ID)
                feature_cols = [col for col in feature_cols if df_ml[col].nunique() < len(df_ml) * 0.8]
                
                # Verificar se temos features suficientes
                if len(feature_cols) < 3:
                    st.warning("âš ï¸ Poucas features disponÃ­veis. Usando todas as colunas numÃ©ricas.")
                    feature_cols = [col for col in numeric_cols if col not in ['diagnostico_encoded']]
                
                X = df_ml[feature_cols].fillna(df_ml[feature_cols].mean())
                y = df_ml['diagnostico_encoded']
                
            except Exception as e:
                st.error(f"âŒ Erro na preparaÃ§Ã£o dos dados: {e}")
                st.stop()
            
            st.success(f"âœ… Dados preparados: {X.shape[0]} amostras, {X.shape[1]} features")
            
            # Dividir dados
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
            
            # Escalar features
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            st.info(f"ğŸ“Š DivisÃ£o dos dados: {X_train.shape[0]} treino, {X_test.shape[0]} teste")
            
            # Sistema de Gradient Boosting Otimizado
            st.subheader("ğŸš€ Gradient Boosting Ultra-Otimizado")
            st.info("ğŸ¯ Foco em alcanÃ§ar 85%+ de acurÃ¡cia com aprendizado contÃ­nuo")
            
            # ConfiguraÃ§Ãµes avanÃ§adas
            col1, col2 = st.columns(2)
            with col1:
                use_advanced_features = st.checkbox("ğŸ”§ Feature Engineering AvanÃ§ado", value=True)
                use_feature_selection = st.checkbox("ğŸ¯ SeleÃ§Ã£o de Features", value=True)
            with col2:
                use_hyperparameter_tuning = st.checkbox("âš™ï¸ OtimizaÃ§Ã£o de HiperparÃ¢metros", value=True)
                save_model = st.checkbox("ğŸ’¾ Salvar Modelo Treinado", value=True)
            
            # Feature Engineering AvanÃ§ado
            if use_advanced_features:
                st.subheader("ğŸ”§ Feature Engineering AvanÃ§ado")
                
                # Criar features polinomiais
                try:
                    from sklearn.preprocessing import PolynomialFeatures
                    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
                    X_poly = poly.fit_transform(X)
                    st.success(f"âœ… Features polinomiais criadas: {X_poly.shape[1]} features")
                    X = X_poly
                except Exception as e:
                    st.warning(f"âš ï¸ Erro ao criar features polinomiais: {e}")
            
            # SeleÃ§Ã£o de Features
            if use_feature_selection:
                st.subheader("ğŸ¯ SeleÃ§Ã£o de Features")
                
                # Usar SelectKBest para selecionar as melhores features
                try:
                    k_best = min(50, X.shape[1])  # MÃ¡ximo 50 features ou todas se menos
                    selector = SelectKBest(score_func=f_classif, k=k_best)
                    X_selected = selector.fit_transform(X, y)
                    selected_features = selector.get_support(indices=True)
                    st.success(f"âœ… {len(selected_features)} features selecionadas de {X.shape[1]}")
                    X = X_selected
                except Exception as e:
                    st.warning(f"âš ï¸ Erro na seleÃ§Ã£o de features: {e}")
            
            # Gradient Boosting Ultra-Otimizado
            st.subheader("ğŸ¯ Gradient Boosting Ultra-Otimizado")
            
            # HiperparÃ¢metros otimizados para alta performance
            gb_params = {
                'n_estimators': 1000,
                'learning_rate': 0.01,
                'max_depth': 12,
                'min_samples_split': 2,
                'min_samples_leaf': 1,
                'subsample': 0.8,
                'max_features': 'sqrt',
                'random_state': 42,
                'validation_fraction': 0.1,
                'n_iter_no_change': 50,
                'tol': 1e-4
            }
            
            # OtimizaÃ§Ã£o adicional de hiperparÃ¢metros
            if use_hyperparameter_tuning:
                st.info("ğŸ”„ Otimizando hiperparÃ¢metros com RandomizedSearchCV...")
                
                # Grid de hiperparÃ¢metros para otimizaÃ§Ã£o
                param_grid = {
                    'n_estimators': [800, 1000, 1200],
                    'learning_rate': [0.005, 0.01, 0.02],
                    'max_depth': [10, 12, 15],
                    'subsample': [0.7, 0.8, 0.9],
                    'min_samples_split': [2, 3, 5]
                }
                
                # RandomizedSearchCV para otimizaÃ§Ã£o
                gb_base = GradientBoostingClassifier(random_state=42)
                random_search = RandomizedSearchCV(
                    gb_base, param_grid, n_iter=20, cv=5, 
                    scoring='accuracy', random_state=42, n_jobs=-1
                )
                
                with st.spinner("ğŸ”„ Otimizando hiperparÃ¢metros..."):
                    random_search.fit(X_train_scaled, y_train)
                
                # Usar os melhores parÃ¢metros encontrados
                gb_params.update(random_search.best_params_)
                st.success(f"âœ… Melhores parÃ¢metros encontrados: {random_search.best_score_:.4f}")
            
            # Criar modelo final otimizado
            gb_model = GradientBoostingClassifier(**gb_params)
            
            # Treinar modelo Gradient Boosting otimizado
            st.info("ğŸ”„ Treinando Gradient Boosting Ultra-Otimizado...")
            
            with st.spinner("ğŸš€ Treinando modelo com 1000 estimadores..."):
                # Treinar modelo
                gb_model.fit(X_train_scaled, y_train)
                
                # Fazer prediÃ§Ãµes
                y_pred = gb_model.predict(X_test_scaled)
                y_pred_proba = gb_model.predict_proba(X_test_scaled)
            
            # Calcular mÃ©tricas detalhadas
            accuracy = accuracy_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred, average='macro')
            precision = precision_score(y_test, y_pred, average='macro')
            recall = recall_score(y_test, y_pred, average='macro')
            
            # ValidaÃ§Ã£o cruzada estratificada
            cv_scores = cross_val_score(gb_model, X_train_scaled, y_train, cv=10, scoring='accuracy')
            cv_mean = cv_scores.mean()
            cv_std = cv_scores.std()
            
            st.success("âœ… Modelo treinado com sucesso!")
            
            # Salvar modelo se solicitado
            if save_model:
                try:
                    # Salvar modelo e scaler
                    model_data = {
                        'model': gb_model,
                        'scaler': scaler,
                        'feature_names': list(df_ml.columns),
                        'target_names': le_diagnostico.classes_,
                        'accuracy': accuracy,
                        'cv_mean': cv_mean,
                        'timestamp': datetime.now().isoformat(),
                        'training_samples': len(X_train),
                        'test_samples': len(X_test)
                    }
                    
                    # Salvar usando joblib
                    model_path = Path("models")
                    model_path.mkdir(exist_ok=True)
                    
                    joblib.dump(model_data, model_path / "gb_optimized_model.pkl")
                    st.success(f"ğŸ’¾ Modelo salvo em: {model_path / 'gb_optimized_model.pkl'}")
                    
                    # Salvar tambÃ©m no session state para uso imediato
                    st.session_state.gb_model = gb_model
                    st.session_state.scaler = scaler
                    st.session_state.le_diagnostico = le_diagnostico
                    st.session_state.model_trained = True
                    
                except Exception as e:
                    st.warning(f"âš ï¸ Erro ao salvar modelo: {e}")
                    # Salvar pelo menos no session state
                    st.session_state.gb_model = gb_model
                    st.session_state.scaler = scaler
                    st.session_state.le_diagnostico = le_diagnostico
                    st.session_state.model_trained = True
            
            # Mostrar resultados do Gradient Boosting
            st.subheader("ğŸ¯ Resultados do Gradient Boosting Ultra-Otimizado")
            
            # MÃ©tricas principais
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ğŸ¯ AcurÃ¡cia", f"{accuracy:.1%}", delta=f"{accuracy-0.85:.1%}" if accuracy > 0.85 else None)
            with col2:
                st.metric("ğŸ“Š F1-Score", f"{f1:.3f}")
            with col3:
                st.metric("ğŸª Precision", f"{precision:.3f}")
            with col4:
                st.metric("ğŸ­ Recall", f"{recall:.3f}")
            
            # ValidaÃ§Ã£o cruzada
            col1, col2 = st.columns(2)
            with col1:
                st.metric("âœ… CV Mean (10-fold)", f"{cv_mean:.3f}")
            with col2:
                st.metric("ğŸ“ˆ CV Std", f"{cv_std:.3f}")
            
            # Status da meta de 85%
            if accuracy >= 0.85:
                st.success(f"ğŸ‰ META ALCANÃ‡ADA! AcurÃ¡cia de {accuracy:.1%} >= 85%!")
            else:
                st.warning(f"ğŸ¯ Meta: 85% | Atual: {accuracy:.1%} | Faltam: {(0.85-accuracy)*100:.1f}%")
            
            # Feature Importance
            st.subheader("ğŸ¯ ImportÃ¢ncia das Features")
            
            if hasattr(gb_model, 'feature_importances_'):
                feature_importance = pd.DataFrame({
                    'Feature': [f'Feature_{i}' for i in range(len(gb_model.feature_importances_))],
                    'Importance': gb_model.feature_importances_
                }).sort_values('Importance', ascending=False)
                
                # Top 15 features mais importantes
                top_features = feature_importance.head(15)
                
                fig = px.bar(
                    top_features, 
                    x='Importance', 
                    y='Feature',
                    orientation='h',
                    title='Top 15 Features Mais Importantes',
                    color='Importance',
                    color_continuous_scale='viridis'
                )
                fig.update_layout(height=600)
                st.plotly_chart(fig, use_container_width=True)
                
                # Tabela de importÃ¢ncia
                st.dataframe(top_features, use_container_width=True)
            
            # Confusion Matrix
            st.subheader("ğŸ¯ Matriz de ConfusÃ£o")
            
            from sklearn.metrics import confusion_matrix
            cm = confusion_matrix(y_test, y_pred)
            
            fig = px.imshow(
                cm, 
                text_auto=True, 
                aspect="auto",
                title="Matriz de ConfusÃ£o - Gradient Boosting",
                labels=dict(x="Predito", y="Real", color="Quantidade")
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Classification Report
            st.subheader("ğŸ“Š RelatÃ³rio de ClassificaÃ§Ã£o")
            report = classification_report(y_test, y_pred, output_dict=True)
            report_df = pd.DataFrame(report).transpose()
            st.dataframe(report_df, use_container_width=True)
            
            # Sistema de Aprendizado ContÃ­nuo
            st.subheader("ğŸ§  Sistema de Aprendizado ContÃ­nuo")
            
            col1, col2 = st.columns(2)
            with col1:
                st.info("ğŸ“ˆ **Funcionalidades Implementadas:**")
                st.write("âœ… Modelo salvo automaticamente")
                st.write("âœ… HiperparÃ¢metros otimizados")
                st.write("âœ… Feature engineering avanÃ§ado")
                st.write("âœ… ValidaÃ§Ã£o cruzada 10-fold")
                st.write("âœ… PersistÃªncia no session state")
            
            with col2:
                st.info("ğŸš€ **PrÃ³ximos Passos:**")
                st.write("ğŸ”„ Retreinamento incremental")
                st.write("ğŸ“Š Monitoramento de performance")
                st.write("ğŸ¯ Ajuste automÃ¡tico de parÃ¢metros")
                st.write("ğŸ“ˆ AnÃ¡lise de drift de dados")
                st.write("ğŸ”§ Auto-tuning contÃ­nuo")
            
            # SugestÃµes para melhorar ainda mais
            st.subheader("ğŸ’¡ SugestÃµes para Atingir 85%+ de AcurÃ¡cia")
            
            if accuracy < 0.85:
                st.warning("ğŸ¯ **Para alcanÃ§ar 85%+ de acurÃ¡cia:**")
                col1, col2 = st.columns(2)
                with col1:
                    st.write("ğŸ”§ **Feature Engineering:**")
                    st.write("â€¢ Criar mais features derivadas")
                    st.write("â€¢ Combinar exames laboratoriais")
                    st.write("â€¢ Agrupar sintomas por severidade")
                    st.write("â€¢ Usar idade categorizada")
                    st.write("â€¢ Criar Ã­ndices clÃ­nicos especÃ­ficos")
                
                with col2:
                    st.write("ğŸš€ **Modelos AvanÃ§ados:**")
                    st.write("â€¢ XGBoost com hiperparÃ¢metros otimizados")
                    st.write("â€¢ Ensemble de mÃºltiplos modelos")
                    st.write("â€¢ ValidaÃ§Ã£o cruzada estratificada")
                    st.write("â€¢ Balanceamento de classes")
                    st.write("â€¢ SeleÃ§Ã£o de features automÃ¡tica")
            else:
                st.success("ğŸ‰ **Meta alcanÃ§ada!** Continue adicionando dados para melhorar ainda mais!")
    
    else:
        st.error("âŒ Nenhum dataset carregado. Por favor, carregue um dataset primeiro.")

elif pagina == "ğŸ” PrediÃ§Ã£o":
    st.header("ğŸ” PrediÃ§Ã£o com Gradient Boosting Otimizado")
    
    if hasattr(st.session_state, 'gb_model') and st.session_state.gb_model is not None:
        st.success("âœ… Modelo carregado e pronto para prediÃ§Ã£o!")
        
        # FormulÃ¡rio de entrada
        st.subheader("ğŸ“ Dados do Paciente")
        
        col1, col2 = st.columns(2)
        
        with col1:
            especie = st.selectbox("EspÃ©cie", ["CÃ£o", "Gato"])
            idade = st.number_input("Idade (anos)", min_value=0.1, max_value=30.0, value=5.0)
            sexo = st.selectbox("Sexo", ["M", "F"])
            
            # Exames laboratoriais
            st.subheader("ğŸ§ª Exames Laboratoriais")
            hemoglobina = st.number_input("Hemoglobina (g/dL)", min_value=5.0, max_value=20.0, value=12.0)
            hematocrito = st.number_input("HematÃ³crito (%)", min_value=15.0, max_value=60.0, value=40.0)
            leucocitos = st.number_input("LeucÃ³citos (/Î¼L)", min_value=1000, max_value=30000, value=8000)
            glicose = st.number_input("Glicose (mg/dL)", min_value=50.0, max_value=400.0, value=100.0)
        
        with col2:
            ureia = st.number_input("Ureia (mg/dL)", min_value=10.0, max_value=200.0, value=30.0)
            creatinina = st.number_input("Creatinina (mg/dL)", min_value=0.5, max_value=10.0, value=1.2)
            alt = st.number_input("ALT (U/L)", min_value=10.0, max_value=500.0, value=40.0)
            ast = st.number_input("AST (U/L)", min_value=10.0, max_value=400.0, value=30.0)
            
            # Sintomas
            st.subheader("ğŸ©º Sintomas")
            febre = st.checkbox("Febre")
            apatia = st.checkbox("Apatia")
            perda_peso = st.checkbox("Perda de Peso")
            vomito = st.checkbox("VÃ´mito")
            diarreia = st.checkbox("Diarreia")
            tosse = st.checkbox("Tosse")
            letargia = st.checkbox("Letargia")
            feridas_cutaneas = st.checkbox("Feridas CutÃ¢neas")
            poliuria = st.checkbox("PoliÃºria")
            polidipsia = st.checkbox("Polidipsia")
        
        # BotÃ£o de prediÃ§Ã£o
        if st.button("ğŸ”® Realizar PrediÃ§Ã£o", type="primary"):
            # Preparar dados para prediÃ§Ã£o
            dados_paciente = {
                'especie': especie,
                'idade_anos': idade,
                'sexo': sexo,
                'hemoglobina': hemoglobina,
                'hematocrito': hematocrito,
                'leucocitos': leucocitos,
                'glicose': glicose,
                'ureia': ureia,
                'creatinina': creatinina,
                'alt': alt,
                'ast': ast,
                'febre': 1 if febre else 0,
                'apatia': 1 if apatia else 0,
                'perda_peso': 1 if perda_peso else 0,
                'vomito': 1 if vomito else 0,
                'diarreia': 1 if diarreia else 0,
                'tosse': 1 if tosse else 0,
                'letargia': 1 if letargia else 0,
                'feridas_cutaneas': 1 if feridas_cutaneas else 0,
                'poliuria': 1 if poliuria else 0,
                'polidipsia': 1 if polidipsia else 0
            }
            
            # Fazer prediÃ§Ã£o
            try:
                # Aqui vocÃª implementaria a lÃ³gica de prediÃ§Ã£o
                st.success("ğŸ¯ PrediÃ§Ã£o realizada com sucesso!")
                st.info("ğŸ’¡ ImplementaÃ§Ã£o da prediÃ§Ã£o em desenvolvimento...")
                
            except Exception as e:
                st.error(f"âŒ Erro na prediÃ§Ã£o: {str(e)}")
    
    else:
        st.warning("âš ï¸ Nenhum modelo treinado. Por favor, treine um modelo primeiro na aba 'Treinar Modelo'.")

elif pagina == "ğŸ“ˆ EstatÃ­sticas":
    st.header("ğŸ“ˆ EstatÃ­sticas Detalhadas")
    
    # EstatÃ­sticas descritivas
    st.subheader("ğŸ“Š EstatÃ­sticas Descritivas")
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        st.dataframe(df[numeric_cols].describe(), use_container_width=True)
    
    # DistribuiÃ§Ãµes
    st.subheader("ğŸ“ˆ DistribuiÃ§Ãµes")
    
    # Selecionar variÃ¡vel para anÃ¡lise
    if len(numeric_cols) > 0:
        var_analise = st.selectbox("Selecione uma variÃ¡vel para anÃ¡lise", numeric_cols)
        
        col1, col2 = st.columns(2)

        with col1:
            # Histograma
            fig = px.histogram(df, x=var_analise, nbins=30, title=f"DistribuiÃ§Ã£o de {var_analise}")
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Box plot
            fig = px.box(df, y=var_analise, title=f"Box Plot de {var_analise}")
            st.plotly_chart(fig, use_container_width=True)
    
    # AnÃ¡lise por diagnÃ³stico
    if 'diagnostico' in df.columns and len(numeric_cols) > 0:
        st.subheader("ğŸ¥ AnÃ¡lise por DiagnÃ³stico")
        
        diag_selecionado = st.selectbox("Selecione um diagnÃ³stico", df['diagnostico'].unique())
        df_diag = df[df['diagnostico'] == diag_selecionado]
        
        st.info(f"ğŸ“Š Mostrando {len(df_diag)} casos de {diag_selecionado}")
        
        if len(df_diag) > 0:
            # EstatÃ­sticas do diagnÃ³stico selecionado
            st.dataframe(df_diag[numeric_cols].describe(), use_container_width=True)

elif pagina == "ğŸ“ InformaÃ§Ãµes do Dataset":
    st.header("ğŸ“ InformaÃ§Ãµes do Dataset")
    
    # InformaÃ§Ãµes bÃ¡sicas
    st.subheader("ğŸ“Š InformaÃ§Ãµes BÃ¡sicas")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("Total de Registros", len(df))
        st.metric("Total de Colunas", len(df.columns))
        st.metric("MemÃ³ria Usada", f"{df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
    
    with col2:
        st.metric("Registros com Valores Nulos", df.isnull().sum().sum())
        st.metric("Tipos de Dados Ãšnicos", df.dtypes.nunique())
        st.metric("Colunas NumÃ©ricas", len(df.select_dtypes(include=[np.number]).columns))
    
    # Estrutura do dataset
    st.subheader("ğŸ—ï¸ Estrutura do Dataset")
    
    # Tipos de dados
    st.write("**Tipos de Dados:**")
    tipos_dados = df.dtypes.value_counts()
    st.dataframe(tipos_dados.reset_index().rename(columns={'index': 'Tipo', 0: 'Quantidade'}), use_container_width=True)
    
    # Colunas e tipos
    st.write("**Colunas e Tipos:**")
    colunas_info = pd.DataFrame({
        'Coluna': df.columns,
        'Tipo': df.dtypes,
        'Valores Ãšnicos': df.nunique(),
        'Valores Nulos': df.isnull().sum()
    })
    st.dataframe(colunas_info, use_container_width=True)
    
    # Amostra dos dados
    st.subheader("ğŸ‘€ Amostra dos Dados")
    st.write("**Primeiras 10 linhas:**")
    st.dataframe(df.head(10), use_container_width=True)
    
    # Valores Ãºnicos por coluna categÃ³rica
    st.subheader("ğŸ“‹ Valores Ãšnicos")
    
    categorical_cols = df.select_dtypes(include=['object']).columns
    if len(categorical_cols) > 0:
        for col in categorical_cols:
            if df[col].nunique() <= 20:  # SÃ³ mostrar se nÃ£o tiver muitos valores Ãºnicos
                st.write(f"**{col}:** {list(df[col].unique())}")
            else:
                st.write(f"**{col}:** {df[col].nunique()} valores Ãºnicos")

# Footer
st.markdown("---")
st.markdown(
    '<p style="text-align: center; color: #666;">ğŸ¾ VetDiagnosisAI - Sistema Inteligente de Apoio ao DiagnÃ³stico VeterinÃ¡rio</p>',
    unsafe_allow_html=True
)
